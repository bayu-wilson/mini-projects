{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "380e125e-6d68-496d-9074-f9860805929f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Neural Networks from scratch\n",
    "\n",
    "https://www.youtube.com/watch?v=Wo5dMEP_BbI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a89b82-78d7-4bbb-837b-ef4144759a7d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### P.1 Intro and Neuron Code\n",
    "\n",
    "We call them neural networks because they look, visually, like neural networks. The neurons have different layers. In the video, there is an input layer, two hidden layers, and an output layer.\n",
    "\n",
    "The end goal of neural networks, like most machine learning, is to take some input data and produce output data that we desire. We can do this by tuning the weights and biases. That is the actual training process. The number of biases is the same as the number of nodes (sum of nodes from each layer combined). A node in one layer is connected to nodes in the next layer by weights. The number of weights is the number of biases in one layer times the number of biases in the next layer plus the same thing for the next layer etc. So the total number of tunable parameters is the number of weights and biases. This ends up being a huge number.\n",
    "\n",
    "The hard part of neural networks and deep learning is figuring out how to tune it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43e314c6-f01a-49be-a251-bd7f7ec99457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python:  3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:29) [Clang 14.0.6 ]\n",
      "Numpy:  1.24.1\n",
      "Matplotlib:  3.6.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "print(\"Python: \",sys.version)\n",
    "print(\"Numpy: \", np.__version__)\n",
    "print(\"Matplotlib: \", matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb67d7c0-95a1-46e5-896a-5135d6ea9276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "inputs = [1,2,3,2.5] #unique inputs from three neurons in the previous layer\n",
    "\n",
    "weights1 = [0.2,0.8,-0.5,1.0] \n",
    "weights2 = [0.5, -0.91, 0.26, -0.5] \n",
    "weights3 = [-0.26, -0.27, 0.17, 0.87] \n",
    "\n",
    "bias1 = 2 #every unique neuron has a unique bias\n",
    "bias2 = 3\n",
    "bias3 = 0.5\n",
    "\n",
    "output = \\\n",
    "[inputs[0]*weights1[0]+inputs[1]*weights1[1]+inputs[2]*weights1[2]+inputs[3]*weights1[3]+bias1,\n",
    " inputs[0]*weights2[0]+inputs[1]*weights2[1]+inputs[2]*weights2[2]+inputs[3]*weights2[3]+bias2,\n",
    " inputs[0]*weights3[0]+inputs[1]*weights3[1]+inputs[2]*weights3[2]+inputs[3]*weights3[3]+bias3]\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c888093-207d-411f-8ef1-70da52cafa27",
   "metadata": {},
   "source": [
    "Why is there only one bias? The inputs are only going into one node. 3 inputs means 3 weights but every neuron has just one bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85cc7eb-fd78-4ed7-8d57-0cfe8b54ca98",
   "metadata": {},
   "source": [
    "### Part 3. The dot product\n",
    "\n",
    "zip combines lists into list of lists, element-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "335bf84c-0f47-458b-a8df-7a293c972670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3, 4), (3,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = [weights1,weights2,weights3] \n",
    "biases = [bias1,bias2,bias3]\n",
    "np.shape(weights),np.shape(biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2046a26c-0811-43d3-bac8-151a05dadb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "layer_outputs = [] #output of current layer\n",
    "for neuron_weights, neuron_bias in zip(weights, biases):\n",
    "    neuron_output = 0 #output of given neuron\n",
    "    for n_input, weight in zip(inputs, neuron_weights):\n",
    "        neuron_output += n_input*weight\n",
    "    neuron_output += neuron_bias\n",
    "    layer_outputs.append(neuron_output)\n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3187b3bf-b3f6-47e6-b25d-cc2384a298e3",
   "metadata": {},
   "source": [
    "Weights and biases are the things that are tuned to fit the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aab8b67-5aed-4caf-9db2-12ea01f31b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.35\n",
      "0.19999999999999996\n"
     ]
    }
   ],
   "source": [
    "some_values = -0.5\n",
    "weight = 0.7\n",
    "bias = 0.7\n",
    "print(some_values*weight) #weight was a multiple of the value\n",
    "print(some_values+bias)   #bias has offset the value\n",
    "#like y=mx+b, b is bias, m is weight, they are two tools that help in different ways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58a3a4d-dfd8-42e8-8fac-3865b376b667",
   "metadata": {},
   "source": [
    "A tensor is an object that can be represented as an array. It is not just an array. A tensor is represented by an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4de0a354-be94-4a2b-9370-d69569904598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.8  , 1.21 , 2.385])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [1,2,3,2.5]\n",
    "weights = [weights1,weights2,weights3] \n",
    "biases = [bias1,bias2,bias3]\n",
    "np.dot(weights,inputs)+biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dca17fd-8e9f-4fd2-b0db-b8225da4e236",
   "metadata": {},
   "source": [
    "### part 4: Batches, layer, and objects\n",
    "\n",
    "Batchesare used because we can calculate them in parallel. The bigger the batch, the more parallel operations we can run. CPUs only have like 4-8 cores. That's not very much. GPUs have hundreds/thousands of cores. The other reason we use batches is to help with generalizations.\n",
    "\n",
    "The size of the batch helps each neuron fit the data. Batch size 32 seems pretty common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bec69aed-b613-4ca3-afa4-2bfaa1241a6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5031 , -1.04185, -2.03875],\n",
       "       [ 0.2434 , -2.7332 , -5.7633 ],\n",
       "       [-0.99314,  1.41254, -0.35655]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = np.array([[1,2,3,2.5],\n",
    "         [2.0,5.0,-1.0,2.0],\n",
    "         [-1.5,2.7,3.3,-0.8]])\n",
    "weights = np.array([weights1,weights2,weights3])\n",
    "biases = [bias1,bias2,bias3]\n",
    "\n",
    "weights_layer2 = np.array([[0.1,-0.14,0.5],\n",
    "                            [-0.5,0.12,-0.33],\n",
    "                            [-0.44,0.73,-0.13]])\n",
    "biases_layer2 = np.array([-1,2,-0.5])\n",
    "\n",
    "layer1_outputs = np.dot(inputs,weights.T)+biases\n",
    "layer2_outputs = np.dot(layer1_outputs,weights_layer2.T)+biases_layer2\n",
    "layer2_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c952377d-d2f4-4f64-8b14-c36678d953ca",
   "metadata": {},
   "source": [
    "In matrix multiplications,\n",
    "$$ Z = XY $$\n",
    "\n",
    "If shape of X is [n,m], the shape of Y must be [m,l]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e559a716-d42e-4315-adc1-9d613e2e2d01",
   "metadata": {},
   "source": [
    "When you save a model, you are just saving the weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0d16a1a-9e08-432f-8410-0f5537980589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.10758131  1.03983522  0.24462411  0.31821498  0.18851053]\n",
      " [-0.08349796  0.70846411  0.00293357  0.44701525  0.36360538]\n",
      " [-0.50763245  0.55688422  0.07987797 -0.34889573  0.04553042]]\n",
      "[[ 0.148296   -0.08397602]\n",
      " [ 0.14100315 -0.01340469]\n",
      " [ 0.20124979 -0.07290616]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "X = np.array([[1,2,3,2.5],\n",
    "             [2.0,5.0,-1.0,2.0],\n",
    "             [-1.5,2.7,3.3,-0.8]])\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.1*np.random.randn(n_inputs,n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    def forward(self,inputs):\n",
    "        self.output = np.dot(inputs,self.weights) + self.biases\n",
    "layer1 = Layer_Dense(4,5) #four inputs and 5 neurons\n",
    "layer1.forward(X) #four inputs. Can have any number of neurons.\n",
    "print(layer1.output)\n",
    "\n",
    "layer2 = Layer_Dense(5,2) # 5 inputs and 2 neurons. Must have same # of inputs as neurons in prev. layer\n",
    "layer2.forward(layer1.output) \n",
    "print(layer2.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2224237f-818e-4fa3-83be-a1ed2c189cec",
   "metadata": {},
   "source": [
    "## Part 5 Activiation function\n",
    "\n",
    "What are they and why are we using them. First we will use a step function as the activation function. Each neuron will have an activation function associated. It comes into play after you do the inputs times the weights plus the bias. The step function will be either 0 or 1.\n",
    "\n",
    "What about a sigmoid activation function? $y=\\frac{1}{1+e^{-x}}$. It's a little easier to train a NN with the sigmoid due to the granularity (smoother?) of the output. The next steps would to be to calculate loss... how wrong is the neural network. Then an optimizer could be use to optimize the weights and bias values to decrease loss. We would need to know the impact of these weights and biases on these individual values. \n",
    "\n",
    "Another option is the rectified linear activation (ReLU) function. The output can be granular. Why use ReLU vs sigmoid? ReLU is simpler and so faster. It also just works. This is the most popular activation function.\n",
    "\n",
    "What is the point of an activation function? Without it, you can't fit non-linear things. ReLU is almost linear, but the rectified part is exatly what makes it powerful and able to fit non-linear data.\n",
    "\n",
    "In order to fit non-linear problems with neural networds, you need two or more hidden layers. And with those layers, you can use non-linear activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47ff04c2-662e-4936-80da-44b14ee61a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "X = np.array([[1,2,3,2.5],\n",
    "             [2.0,5.0,-1.0,2.0],\n",
    "             [-1.5,2.7,3.3,-0.8]])\n",
    "\n",
    "inputs = [0,2,-1,3.3,-2.7,1.1,2.2,-100]\n",
    "output = []\n",
    "\n",
    "for i in inputs:\n",
    "    output.append(max(0,i))\n",
    "    # if i >0:\n",
    "    #     output.append(i)\n",
    "    # elif i<=0:\n",
    "    #     output.append(0)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "edab3b93-1d13-4bf6-b867-e30147cf1483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 4.65504505e-04\n",
      "  4.56846210e-05]\n",
      " [0.00000000e+00 5.93469958e-05 0.00000000e+00 2.03573116e-04\n",
      "  6.10024377e-04]\n",
      " ...\n",
      " [1.13291524e-01 0.00000000e+00 0.00000000e+00 8.11079666e-02\n",
      "  0.00000000e+00]\n",
      " [1.34588361e-01 0.00000000e+00 3.09493970e-02 5.66337556e-02\n",
      "  0.00000000e+00]\n",
      " [1.07817926e-01 0.00000000e+00 0.00000000e+00 8.72561932e-02\n",
      "  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "# X = np.array([[1,2,3,2.5],\n",
    "#              [2.0,5.0,-1.0,2.0],\n",
    "#              [-1.5,2.7,3.3,-0.8]])\n",
    "X,y = spiral_data(100,3)\n",
    "\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.1*np.random.randn(n_inputs,n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "    def forward(self,inputs):\n",
    "        self.output = np.dot(inputs,self.weights) + self.biases\n",
    "        \n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0,inputs)\n",
    "        \n",
    "layer1 = Layer_Dense(2,5) #2 unique features decribe the results\n",
    "activation1 = Activation_ReLU()\n",
    "layer1.forward(X)\n",
    "activation1.forward(layer1.output)\n",
    "print(activation1.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "17670387-c032-4d4d-a3e7-c8e5f2fcbc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.shape(activation1.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3f7fac-35c0-467e-a3dc-edc8aef7ac18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6321f64a-7051-466e-a796-e99d96804eb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
