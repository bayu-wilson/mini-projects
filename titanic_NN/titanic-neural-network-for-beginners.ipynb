{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "82271100e26c1319f3587faf62b98b6097c9d538"
   },
   "source": [
    "# Titanic challenge part 3\n",
    "In this notebook, we will be covering all of the steps required to train, tune and assess a neural network.\n",
    "\n",
    "**[Part 1](https://www.kaggle.com/jamesleslie/titanic-eda-wrangling-imputation)** of this series dealt with the pre-processing and manipulation of the data. This notebook will make use of the datasets that were created in the first part.\n",
    "\n",
    "We will do each of the following:\n",
    "- train and test a neural network model\n",
    "- use grid search to optimize the hyperparameters\n",
    "- submit predictions for the test set\n",
    "\n",
    "**[Part 2](https://www.kaggle.com/jamesleslie/titanic-random-forest-grid-search)** covered the use of a random forest for tackling this challenge. Now let's see if we can beat that model with a neural network!\n",
    "> NOTE: make sure to use a GPU for this notebook, as it will be significantly faster to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "969e5a36-8f64-4129-ba03-7fd19b314ca9",
    "_uuid": "cc4b58927770beab136969e526dbbd69a9cc21c8"
   },
   "source": [
    "# Table of Contents:\n",
    "\n",
    "- **1. [Load packages and data](#loading)**\n",
    "- **2. [Pre-processing](#Pre-processing)**\n",
    "  - **2.1. [Variable Encoding](#encoding)**\n",
    "  - **2.2. [Variable Scaling](#scaling)**\n",
    "- **3. [Neural Network](#Neural Network)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ec3a4ceb-2397-45b3-aafe-a8ffde879888",
    "_uuid": "de8369a5716eb80519979ef773ecbb135f66e4b9"
   },
   "source": [
    "<a id=\"loading\"></a>\n",
    "# 1. Load packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-output": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'set_random_seed' from 'tensorflow' (/Users/bayuwilson/miniforge3/lib/python3.10/site-packages/tensorflow/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Activation, Dropout\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrandom\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m seed\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m set_random_seed\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'set_random_seed' from 'tensorflow' (/Users/bayuwilson/miniforge3/lib/python3.10/site-packages/tensorflow/__init__.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import rcParams\n",
    "%matplotlib inline\n",
    "rcParams['figure.figsize'] = 10,8\n",
    "sns.set(style='whitegrid', palette='muted',\n",
    "        rc={'figure.figsize': (15,10)})\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "\n",
    "from numpy.random import seed\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "# print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e4a7f976fc617dd1563cdcf55e6159bacaba1186"
   },
   "outputs": [],
   "source": [
    "print(os.listdir(\"../input/titanic-cleaned-data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-output": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Load data as Pandas dataframe\n",
    "train = pd.read_csv('../input/titanic-cleaned-data/train_clean.csv', )\n",
    "test = pd.read_csv('../input/titanic-cleaned-data/test_clean.csv')\n",
    "df = pd.concat([train, test], axis=0, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bafdb419c8b7f571dbbb9c0b63fd3c52c66c3c1b"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d8f24917231bd2484e4c2a4896f4fbbdcc3e89ab"
   },
   "outputs": [],
   "source": [
    "def display_all(df):\n",
    "    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): \n",
    "        display(df)\n",
    "\n",
    "        \n",
    "display_all(df.describe(include='all').T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1fc54e6-9655-4a05-b147-e2dfe206c7d0",
    "_uuid": "732fc7427f32d790561d03b4a6d870d2a7c67013"
   },
   "source": [
    "<a id=\"pre-processing\"></a>\n",
    "# 2. Pre-processing\n",
    "<a id=\"encoding\"></a>\n",
    "## 2.1. Encode Categorical Variables\n",
    "We need to convert all categorical variables into numeric format. The categorical variables we will be keeping are `Embarked`, `Sex` and `Title`.   \n",
    "\n",
    "The `Sex` variable can be encoded into single 1-or-0 column, but the other variables will need to be [one-hot encoded](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f). Regular label encoding assigns some category labels higher numerical values. This implies some sort of scale (Embarked = 1 is not **more** than Embarked = 0 - it's just _different_). One Hot Encoding avoids this problem.   \n",
    "\n",
    "We will assume that there is some ordinality in the `Pclass` variable, so we will leave that as a single column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7d6d38029e57b2b7eecd8978b4f3b9ab2bbf79d9"
   },
   "outputs": [],
   "source": [
    "sns.countplot(x='Pclass', data=df, palette='hls', hue='Survived')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3b5e6978df59b98a614297659371823a4a5dbbcc"
   },
   "outputs": [],
   "source": [
    "sns.countplot(x='Sex', data=df, palette='hls', hue='Survived')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "74e4e4d6a937c94fda45dcd135c8f2936707c8d6"
   },
   "outputs": [],
   "source": [
    "sns.countplot(x='Embarked', data=df, palette='hls', hue='Survived')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e4350c9dbdfedb4a92d266b30daa3e0e35f3f322"
   },
   "outputs": [],
   "source": [
    "# convert to cateogry dtype\n",
    "df['Sex'] = df['Sex'].astype('category')\n",
    "# convert to category codes\n",
    "df['Sex'] = df['Sex'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ddf8a643-1c1d-4036-818c-0d230a05310d",
    "_uuid": "911939a11430319dbda5a2c533c6b8c077a46766"
   },
   "outputs": [],
   "source": [
    "# subset all categorical variables which need to be encoded\n",
    "categorical = ['Embarked', 'Title']\n",
    "\n",
    "for var in categorical:\n",
    "    df = pd.concat([df, \n",
    "                    pd.get_dummies(df[var], prefix=var)], axis=1)\n",
    "    del df[var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ecfe8fb0453e91f3a8cf80c45ed8ad4a406e0e0c"
   },
   "outputs": [],
   "source": [
    "# drop the variables we won't be using\n",
    "df.drop(['Cabin', 'Name', 'Ticket', 'PassengerId'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "416083bad67fefd62cfee27da0f60bdb57a9540b"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "93184d3e-b8d2-4ff9-8ca1-1bcc0330b26b",
    "_uuid": "49cdd41345edc32fc7b4441efa558fc39fd5a1f2"
   },
   "source": [
    "## 2.2. Scale Continuous Variables\n",
    "The continuous variables need to be scaled. This is done using a standard scaler from SkLearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2a32ec90f5150897398f01bd760b6ecbc959e451"
   },
   "outputs": [],
   "source": [
    "continuous = ['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Family_Size']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "for var in continuous:\n",
    "    df[var] = df[var].astype('float64')\n",
    "    df[var] = scaler.fit_transform(df[var].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "48b3f476bfd1b6b307560d0f7a81122fd7b4f2bd"
   },
   "outputs": [],
   "source": [
    "display_all(df.describe(include='all').T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bc50ae8f-a1d7-48b6-9771-e50b4e8e169b",
    "_uuid": "bbead52ea8dd6e9cba5da969375c7829a34b87d4"
   },
   "source": [
    "<a id=\"neural-network\"></a>\n",
    "# 3. Neural Network\n",
    "Now, all that is left is to feed our data that has been cleaned, encoded and scaled to our neural network.\n",
    "\n",
    "But first, we need to separate *data_df* back into *train* and *test* sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c3fffdde68114df12c2a1871d8501cae18a2e2ce"
   },
   "outputs": [],
   "source": [
    "X_train = df[pd.notnull(df['Survived'])].drop(['Survived'], axis=1)\n",
    "y_train = df[pd.notnull(df['Survived'])]['Survived']\n",
    "X_test = df[pd.isnull(df['Survived'])].drop(['Survived'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bc50ae8f-a1d7-48b6-9771-e50b4e8e169b",
    "_uuid": "bbead52ea8dd6e9cba5da969375c7829a34b87d4"
   },
   "source": [
    "## 3.1. Cross-validation\n",
    "Keras allows us to make use of cross-validation for training our model. So we will use this to train and assess our first model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "edcc8edb-a50b-4ec3-9958-06c081fbcd68",
    "_uuid": "75f56351057f33dcee93e207c4b3c461fcb65fae"
   },
   "source": [
    "### Create neural network model\n",
    "For this task, I have kept the model architecture pretty simple. We have one input layer with 17 nodes which feeds into a hidden layer with 8 nodes and an output layer which is used to predict a passenger's survival.   \n",
    "\n",
    "The output layer has a sigmoid activation function, which is used to 'squash' all our outputs to be between 0 and 1.   \n",
    "\n",
    "We are going to create a function which allows to parameterise the choice of hyperparameters in the neural network. This might seem a little overly complicated now, but it will come in super handy when we move onto tuning our parameters later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7b320adc3b344552853708b033e677a4b2cca3f7"
   },
   "outputs": [],
   "source": [
    "def create_model(lyrs=[8], act='linear', opt='Adam', dr=0.0):\n",
    "    \n",
    "    # set random seed for reproducibility\n",
    "    seed(42)\n",
    "    set_random_seed(42)\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # create first hidden layer\n",
    "    model.add(Dense(lyrs[0], input_dim=X_train.shape[1], activation=act))\n",
    "    \n",
    "    # create additional hidden layers\n",
    "    for i in range(1,len(lyrs)):\n",
    "        model.add(Dense(lyrs[i], activation=act))\n",
    "    \n",
    "    # add dropout, default is none\n",
    "    model.add(Dropout(dr))\n",
    "    \n",
    "    # create output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))  # output layer\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7f044d493a62e15bfb1e06a7f792ab53a3df7a0a"
   },
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "34a52736-364d-4f77-87c1-b432a8cc6834",
    "_uuid": "1d96485d799ad3e3850dfc5656680dec1b62ae95"
   },
   "source": [
    "### Train model\n",
    "At this stage, we have our model. We have chosen a few hyperparameters such as the number of hidden layers, the number of neurons and the activation function.\n",
    "\n",
    "The next step is to train the model on our training set. This step also requires us to choose a few more hyperparameters such as the loss function, the optimization algorithm, the number of epochs and the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8b7b5329d5fb74520ea54ce7ebb6f77bcc1f698c"
   },
   "outputs": [],
   "source": [
    "# train model on full train set, with 80/20 CV split\n",
    "training = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=0)\n",
    "val_acc = np.mean(training.history['val_acc'])\n",
    "print(\"\\n%s: %.2f%%\" % ('val_acc', val_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e32fca56ce00a0b1ad97cd65a8a9151fcc89470d"
   },
   "source": [
    "### Assess results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "649c26f2052b5c40e387eaaf985cd894ba0d98b4"
   },
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(training.history['acc'])\n",
    "plt.plot(training.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "eabfa8a3dbb1dd061e642e8263bcf98feae6e65e"
   },
   "source": [
    "## 3.2. Grid search\n",
    "### 3.2.1. batch size and epochs\n",
    "We can see from the graph above that we might be training our network for too long. Let's use **grid search** to find out what the optimal values for `batch_size` and `epochs` are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aee8c8a76611d4e075179b2191b4da5324c472bd"
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = [16, 32, 64]\n",
    "epochs = [50, 100]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "# search the grid\n",
    "grid = GridSearchCV(estimator=model, \n",
    "                    param_grid=param_grid,\n",
    "                    cv=3,\n",
    "                    verbose=2)  # include n_jobs=-1 if you are using CPU\n",
    "\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aee8c8a76611d4e075179b2191b4da5324c472bd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "186635a5b21616833ab080a6d311d20345845acc"
   },
   "source": [
    "### 3.2.2. Optimization Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d6cafd5c7f3ccb0a2b1fa95b8f77ebd5c8ad664e"
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Nadam']\n",
    "param_grid = dict(opt=optimizer)\n",
    "\n",
    "# search the grid\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, verbose=2)\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "78ab745d56a5422ddfffa5ee525738f191ef4f28"
   },
   "outputs": [],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "abe56e7632bdd2d6ee8da250ae0b3333481a989e"
   },
   "source": [
    "### 3.2.3. Hidden neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0c0261b56338bbf5ec508fdee4628e243098c2c9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed(42)\n",
    "set_random_seed(42)\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, \n",
    "                        epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "layers = [[8],[10],[10,5],[12,6],[12,8,4]]\n",
    "param_grid = dict(lyrs=layers)\n",
    "\n",
    "# search the grid\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, verbose=2)\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "78ab745d56a5422ddfffa5ee525738f191ef4f28"
   },
   "outputs": [],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7b629c59eebd4242e919c935c550fe4240317e9a"
   },
   "source": [
    "### 3.2.4. Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0c0261b56338bbf5ec508fdee4628e243098c2c9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, \n",
    "                        epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "drops = [0.0, 0.01, 0.05, 0.1, 0.2, 0.5]\n",
    "param_grid = dict(dr=drops)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, verbose=2)\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8001b384e19fc963a3936b10b5be571af1c06b2e"
   },
   "outputs": [],
   "source": [
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9383e7b646491d4333ff1ccb27c785b794dbc323"
   },
   "outputs": [],
   "source": [
    "# create final model\n",
    "model = create_model(lyrs=[8], dr=0.2)\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b26ec6f0d56e5c48a46f16bdf230ae0483a2d21f"
   },
   "outputs": [],
   "source": [
    "# train model on full train set, with 80/20 CV split\n",
    "training = model.fit(X_train, y_train, epochs=50, batch_size=32, \n",
    "                     validation_split=0.2, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_train, y_train)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4515e41b912a66d382a23708137d1e7086dfdd66"
   },
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(training.history['acc'])\n",
    "plt.plot(training.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5ccf334a-8a72-45e5-b565-cc516639e087",
    "_uuid": "ae291559b10db2f627a87f62a553d15da43ada31"
   },
   "source": [
    "## 3.3. Make Predictions on Test Set\n",
    "Finally, we can attempt to predict which passengers in the test set survived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "48508e7a-cc0f-4c8d-a102-57451d0371de",
    "_uuid": "b40dfc26258b3a73d956db1d697c85caf9ad97bb"
   },
   "outputs": [],
   "source": [
    "# calculate predictions\n",
    "test['Survived'] = model.predict(X_test)\n",
    "test['Survived'] = test['Survived'].apply(lambda x: round(x,0)).astype('int')\n",
    "solution = test[['PassengerId', 'Survived']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c745a7f0663c1ef0cecef30bd7a38daff6420354"
   },
   "outputs": [],
   "source": [
    "solution.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0f646038-2eb3-4a77-94f5-a3abc6c3be1f",
    "_uuid": "0a9b9f4189c03b253c9140c4a93c9f453508a6ba"
   },
   "source": [
    "## 3.4. Output Final Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "04e4be98-5955-43ab-a355-be65580a1162",
    "_uuid": "7dc52ff626a2620c8607c25d85bd8952f049690b"
   },
   "outputs": [],
   "source": [
    "solution.to_csv(\"Neural_Network_Solution.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
